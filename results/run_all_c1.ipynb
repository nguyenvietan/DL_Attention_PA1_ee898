{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_all_c1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenvietan/DL_Attention_PA1_ee898/blob/master/run_all_c1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn3L5K6mngd4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3107
        },
        "outputId": "3e0e7fbc-aed8-4aac-ebd1-8f757b271b30"
      },
      "source": [
        "!bash run_all_c1.sh"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(arch='bam_resnet50_c', batch_size=128, epoch=100, learning_rate=0.1)\n",
            "Model:  bam_resnet50_c\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[Epoch   0], lr: 0.10000, Loss: 4.823 | top1-e-train: 0.975, top5-e-train: 0.898 | top1-e-test: 0.972, top5-e-test: 0.898\n",
            "[Epoch   1], lr: 0.10000, Loss: 4.289 | top1-e-train: 0.944, top5-e-train: 0.781 | top1-e-test: 0.945, top5-e-test: 0.776\n",
            "[Epoch   2], lr: 0.10000, Loss: 3.938 | top1-e-train: 0.881, top5-e-train: 0.649 | top1-e-test: 0.882, top5-e-test: 0.638\n",
            "[Epoch   3], lr: 0.10000, Loss: 3.570 | top1-e-train: 0.821, top5-e-train: 0.543 | top1-e-test: 0.820, top5-e-test: 0.547\n",
            "[Epoch   4], lr: 0.10000, Loss: 3.236 | top1-e-train: 0.758, top5-e-train: 0.457 | top1-e-test: 0.744, top5-e-test: 0.452\n",
            "[Epoch   5], lr: 0.10000, Loss: 2.980 | top1-e-train: 0.719, top5-e-train: 0.398 | top1-e-test: 0.706, top5-e-test: 0.399\n",
            "[Epoch   6], lr: 0.10000, Loss: 2.745 | top1-e-train: 0.672, top5-e-train: 0.351 | top1-e-test: 0.665, top5-e-test: 0.353\n",
            "[Epoch   7], lr: 0.10000, Loss: 2.529 | top1-e-train: 0.637, top5-e-train: 0.317 | top1-e-test: 0.641, top5-e-test: 0.334\n",
            "[Epoch   8], lr: 0.10000, Loss: 2.359 | top1-e-train: 0.613, top5-e-train: 0.279 | top1-e-test: 0.621, top5-e-test: 0.298\n",
            "[Epoch   9], lr: 0.10000, Loss: 2.198 | top1-e-train: 0.560, top5-e-train: 0.240 | top1-e-test: 0.579, top5-e-test: 0.268\n",
            "[Epoch  10], lr: 0.10000, Loss: 2.074 | top1-e-train: 0.532, top5-e-train: 0.221 | top1-e-test: 0.561, top5-e-test: 0.255\n",
            "[Epoch  11], lr: 0.10000, Loss: 1.960 | top1-e-train: 0.503, top5-e-train: 0.195 | top1-e-test: 0.541, top5-e-test: 0.241\n",
            "[Epoch  12], lr: 0.10000, Loss: 1.855 | top1-e-train: 0.470, top5-e-train: 0.172 | top1-e-test: 0.520, top5-e-test: 0.220\n",
            "[Epoch  13], lr: 0.10000, Loss: 1.757 | top1-e-train: 0.466, top5-e-train: 0.169 | top1-e-test: 0.528, top5-e-test: 0.224\n",
            "[Epoch  14], lr: 0.10000, Loss: 1.669 | top1-e-train: 0.447, top5-e-train: 0.154 | top1-e-test: 0.513, top5-e-test: 0.222\n",
            "[Epoch  15], lr: 0.10000, Loss: 1.587 | top1-e-train: 0.419, top5-e-train: 0.136 | top1-e-test: 0.496, top5-e-test: 0.202\n",
            "[Epoch  16], lr: 0.10000, Loss: 1.514 | top1-e-train: 0.384, top5-e-train: 0.115 | top1-e-test: 0.484, top5-e-test: 0.191\n",
            "[Epoch  17], lr: 0.10000, Loss: 1.435 | top1-e-train: 0.371, top5-e-train: 0.112 | top1-e-test: 0.480, top5-e-test: 0.197\n",
            "[Epoch  18], lr: 0.10000, Loss: 1.354 | top1-e-train: 0.350, top5-e-train: 0.096 | top1-e-test: 0.474, top5-e-test: 0.186\n",
            "[Epoch  19], lr: 0.10000, Loss: 1.305 | top1-e-train: 0.330, top5-e-train: 0.087 | top1-e-test: 0.463, top5-e-test: 0.187\n",
            "[Epoch  20], lr: 0.10000, Loss: 1.224 | top1-e-train: 0.311, top5-e-train: 0.075 | top1-e-test: 0.454, top5-e-test: 0.180\n",
            "[Epoch  21], lr: 0.10000, Loss: 1.159 | top1-e-train: 0.305, top5-e-train: 0.069 | top1-e-test: 0.461, top5-e-test: 0.186\n",
            "[Epoch  22], lr: 0.10000, Loss: 1.091 | top1-e-train: 0.282, top5-e-train: 0.059 | top1-e-test: 0.453, top5-e-test: 0.177\n",
            "[Epoch  23], lr: 0.10000, Loss: 1.038 | top1-e-train: 0.279, top5-e-train: 0.060 | top1-e-test: 0.451, top5-e-test: 0.180\n",
            "[Epoch  24], lr: 0.10000, Loss: 1.006 | top1-e-train: 0.293, top5-e-train: 0.062 | top1-e-test: 0.473, top5-e-test: 0.186\n",
            "[Epoch  25], lr: 0.10000, Loss: 0.956 | top1-e-train: 0.234, top5-e-train: 0.042 | top1-e-test: 0.438, top5-e-test: 0.169\n",
            "[Epoch  26], lr: 0.10000, Loss: 0.859 | top1-e-train: 0.228, top5-e-train: 0.041 | top1-e-test: 0.446, top5-e-test: 0.171\n",
            "[Epoch  27], lr: 0.10000, Loss: 0.804 | top1-e-train: 0.210, top5-e-train: 0.034 | top1-e-test: 0.442, top5-e-test: 0.168\n",
            "[Epoch  28], lr: 0.10000, Loss: 0.751 | top1-e-train: 0.192, top5-e-train: 0.027 | top1-e-test: 0.444, top5-e-test: 0.166\n",
            "[Epoch  29], lr: 0.01000, Loss: 0.460 | top1-e-train: 0.103, top5-e-train: 0.014 | top1-e-test: 0.404, top5-e-test: 0.147\n",
            "[Epoch  30], lr: 0.01000, Loss: 0.361 | top1-e-train: 0.085, top5-e-train: 0.010 | top1-e-test: 0.401, top5-e-test: 0.147\n",
            "[Epoch  31], lr: 0.01000, Loss: 0.308 | top1-e-train: 0.076, top5-e-train: 0.009 | top1-e-test: 0.401, top5-e-test: 0.147\n",
            "[Epoch  32], lr: 0.01000, Loss: 0.278 | top1-e-train: 0.068, top5-e-train: 0.008 | top1-e-test: 0.396, top5-e-test: 0.147\n",
            "[Epoch  33], lr: 0.01000, Loss: 0.256 | top1-e-train: 0.066, top5-e-train: 0.010 | top1-e-test: 0.399, top5-e-test: 0.146\n",
            "[Epoch  34], lr: 0.01000, Loss: 0.229 | top1-e-train: 0.055, top5-e-train: 0.006 | top1-e-test: 0.396, top5-e-test: 0.149\n",
            "[Epoch  35], lr: 0.01000, Loss: 0.217 | top1-e-train: 0.052, top5-e-train: 0.009 | top1-e-test: 0.395, top5-e-test: 0.150\n",
            "[Epoch  36], lr: 0.01000, Loss: 0.199 | top1-e-train: 0.049, top5-e-train: 0.009 | top1-e-test: 0.399, top5-e-test: 0.150\n",
            "[Epoch  37], lr: 0.01000, Loss: 0.180 | top1-e-train: 0.041, top5-e-train: 0.004 | top1-e-test: 0.400, top5-e-test: 0.150\n",
            "[Epoch  38], lr: 0.01000, Loss: 0.171 | top1-e-train: 0.044, top5-e-train: 0.010 | top1-e-test: 0.399, top5-e-test: 0.149\n",
            "[Epoch  39], lr: 0.01000, Loss: 0.160 | top1-e-train: 0.041, top5-e-train: 0.009 | top1-e-test: 0.399, top5-e-test: 0.153\n",
            "[Epoch  40], lr: 0.01000, Loss: 0.153 | top1-e-train: 0.035, top5-e-train: 0.007 | top1-e-test: 0.400, top5-e-test: 0.152\n",
            "[Epoch  41], lr: 0.01000, Loss: 0.156 | top1-e-train: 0.038, top5-e-train: 0.009 | top1-e-test: 0.400, top5-e-test: 0.152\n",
            "[Epoch  42], lr: 0.01000, Loss: 0.142 | top1-e-train: 0.031, top5-e-train: 0.004 | top1-e-test: 0.396, top5-e-test: 0.148\n",
            "[Epoch  43], lr: 0.01000, Loss: 0.131 | top1-e-train: 0.027, top5-e-train: 0.004 | top1-e-test: 0.401, top5-e-test: 0.149\n",
            "[Epoch  44], lr: 0.01000, Loss: 0.119 | top1-e-train: 0.028, top5-e-train: 0.007 | top1-e-test: 0.397, top5-e-test: 0.150\n",
            "[Epoch  45], lr: 0.01000, Loss: 0.113 | top1-e-train: 0.025, top5-e-train: 0.006 | top1-e-test: 0.402, top5-e-test: 0.150\n",
            "[Epoch  46], lr: 0.01000, Loss: 0.103 | top1-e-train: 0.030, top5-e-train: 0.010 | top1-e-test: 0.402, top5-e-test: 0.154\n",
            "[Epoch  47], lr: 0.01000, Loss: 0.103 | top1-e-train: 0.023, top5-e-train: 0.004 | top1-e-test: 0.396, top5-e-test: 0.152\n",
            "[Epoch  48], lr: 0.01000, Loss: 0.096 | top1-e-train: 0.021, top5-e-train: 0.004 | top1-e-test: 0.398, top5-e-test: 0.154\n",
            "[Epoch  49], lr: 0.01000, Loss: 0.088 | top1-e-train: 0.021, top5-e-train: 0.004 | top1-e-test: 0.397, top5-e-test: 0.155\n",
            "[Epoch  50], lr: 0.01000, Loss: 0.084 | top1-e-train: 0.019, top5-e-train: 0.005 | top1-e-test: 0.396, top5-e-test: 0.153\n",
            "[Epoch  51], lr: 0.01000, Loss: 0.085 | top1-e-train: 0.022, top5-e-train: 0.007 | top1-e-test: 0.396, top5-e-test: 0.155\n",
            "[Epoch  52], lr: 0.01000, Loss: 0.077 | top1-e-train: 0.017, top5-e-train: 0.004 | top1-e-test: 0.398, top5-e-test: 0.154\n",
            "[Epoch  53], lr: 0.01000, Loss: 0.075 | top1-e-train: 0.015, top5-e-train: 0.002 | top1-e-test: 0.399, top5-e-test: 0.153\n",
            "[Epoch  54], lr: 0.01000, Loss: 0.069 | top1-e-train: 0.014, top5-e-train: 0.003 | top1-e-test: 0.401, top5-e-test: 0.152\n",
            "[Epoch  55], lr: 0.01000, Loss: 0.069 | top1-e-train: 0.013, top5-e-train: 0.003 | top1-e-test: 0.399, top5-e-test: 0.152\n",
            "[Epoch  56], lr: 0.01000, Loss: 0.064 | top1-e-train: 0.015, top5-e-train: 0.005 | top1-e-test: 0.397, top5-e-test: 0.151\n",
            "[Epoch  57], lr: 0.01000, Loss: 0.061 | top1-e-train: 0.016, top5-e-train: 0.007 | top1-e-test: 0.396, top5-e-test: 0.151\n",
            "[Epoch  58], lr: 0.01000, Loss: 0.060 | top1-e-train: 0.014, top5-e-train: 0.006 | top1-e-test: 0.397, top5-e-test: 0.153\n",
            "[Epoch  59], lr: 0.00100, Loss: 0.052 | top1-e-train: 0.015, top5-e-train: 0.008 | top1-e-test: 0.394, top5-e-test: 0.153\n",
            "[Epoch  60], lr: 0.00100, Loss: 0.047 | top1-e-train: 0.016, top5-e-train: 0.009 | top1-e-test: 0.393, top5-e-test: 0.151\n",
            "[Epoch  61], lr: 0.00100, Loss: 0.045 | top1-e-train: 0.011, top5-e-train: 0.004 | top1-e-test: 0.393, top5-e-test: 0.151\n",
            "[Epoch  62], lr: 0.00100, Loss: 0.044 | top1-e-train: 0.012, top5-e-train: 0.006 | top1-e-test: 0.396, top5-e-test: 0.151\n",
            "[Epoch  63], lr: 0.00100, Loss: 0.043 | top1-e-train: 0.012, top5-e-train: 0.006 | top1-e-test: 0.394, top5-e-test: 0.151\n",
            "[Epoch  64], lr: 0.00100, Loss: 0.041 | top1-e-train: 0.010, top5-e-train: 0.004 | top1-e-test: 0.391, top5-e-test: 0.151\n",
            "[Epoch  65], lr: 0.00100, Loss: 0.040 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.392, top5-e-test: 0.153\n",
            "[Epoch  66], lr: 0.00100, Loss: 0.041 | top1-e-train: 0.013, top5-e-train: 0.007 | top1-e-test: 0.391, top5-e-test: 0.152\n",
            "[Epoch  67], lr: 0.00100, Loss: 0.039 | top1-e-train: 0.010, top5-e-train: 0.004 | top1-e-test: 0.392, top5-e-test: 0.151\n",
            "[Epoch  68], lr: 0.00100, Loss: 0.039 | top1-e-train: 0.010, top5-e-train: 0.004 | top1-e-test: 0.391, top5-e-test: 0.152\n",
            "[Epoch  69], lr: 0.00100, Loss: 0.037 | top1-e-train: 0.011, top5-e-train: 0.005 | top1-e-test: 0.391, top5-e-test: 0.151\n",
            "[Epoch  70], lr: 0.00100, Loss: 0.037 | top1-e-train: 0.007, top5-e-train: 0.002 | top1-e-test: 0.391, top5-e-test: 0.151\n",
            "[Epoch  71], lr: 0.00100, Loss: 0.036 | top1-e-train: 0.013, top5-e-train: 0.007 | top1-e-test: 0.393, top5-e-test: 0.152\n",
            "[Epoch  72], lr: 0.00100, Loss: 0.037 | top1-e-train: 0.008, top5-e-train: 0.003 | top1-e-test: 0.393, top5-e-test: 0.152\n",
            "[Epoch  73], lr: 0.00100, Loss: 0.036 | top1-e-train: 0.009, top5-e-train: 0.004 | top1-e-test: 0.391, top5-e-test: 0.153\n",
            "[Epoch  74], lr: 0.00100, Loss: 0.033 | top1-e-train: 0.012, top5-e-train: 0.007 | top1-e-test: 0.393, top5-e-test: 0.154\n",
            "[Epoch  75], lr: 0.00100, Loss: 0.034 | top1-e-train: 0.008, top5-e-train: 0.003 | top1-e-test: 0.392, top5-e-test: 0.152\n",
            "[Epoch  76], lr: 0.00100, Loss: 0.035 | top1-e-train: 0.011, top5-e-train: 0.007 | top1-e-test: 0.390, top5-e-test: 0.153\n",
            "[Epoch  77], lr: 0.00100, Loss: 0.035 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.394, top5-e-test: 0.152\n",
            "[Epoch  78], lr: 0.00100, Loss: 0.034 | top1-e-train: 0.008, top5-e-train: 0.003 | top1-e-test: 0.395, top5-e-test: 0.154\n",
            "[Epoch  79], lr: 0.00100, Loss: 0.035 | top1-e-train: 0.014, top5-e-train: 0.009 | top1-e-test: 0.393, top5-e-test: 0.155\n",
            "[Epoch  80], lr: 0.00100, Loss: 0.032 | top1-e-train: 0.010, top5-e-train: 0.006 | top1-e-test: 0.394, top5-e-test: 0.152\n",
            "[Epoch  81], lr: 0.00100, Loss: 0.032 | top1-e-train: 0.008, top5-e-train: 0.004 | top1-e-test: 0.391, top5-e-test: 0.152\n",
            "[Epoch  82], lr: 0.00100, Loss: 0.034 | top1-e-train: 0.009, top5-e-train: 0.005 | top1-e-test: 0.393, top5-e-test: 0.152\n",
            "[Epoch  83], lr: 0.00100, Loss: 0.032 | top1-e-train: 0.011, top5-e-train: 0.007 | top1-e-test: 0.393, top5-e-test: 0.151\n",
            "[Epoch  84], lr: 0.00100, Loss: 0.032 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.390, top5-e-test: 0.154\n",
            "[Epoch  85], lr: 0.00100, Loss: 0.030 | top1-e-train: 0.010, top5-e-train: 0.006 | top1-e-test: 0.393, top5-e-test: 0.154\n",
            "[Epoch  86], lr: 0.00100, Loss: 0.031 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.393, top5-e-test: 0.153\n",
            "[Epoch  87], lr: 0.00100, Loss: 0.032 | top1-e-train: 0.010, top5-e-train: 0.006 | top1-e-test: 0.392, top5-e-test: 0.152\n",
            "[Epoch  88], lr: 0.00100, Loss: 0.030 | top1-e-train: 0.012, top5-e-train: 0.008 | top1-e-test: 0.392, top5-e-test: 0.152\n",
            "[Epoch  89], lr: 0.00010, Loss: 0.029 | top1-e-train: 0.008, top5-e-train: 0.004 | top1-e-test: 0.394, top5-e-test: 0.151\n",
            "[Epoch  90], lr: 0.00010, Loss: 0.030 | top1-e-train: 0.008, top5-e-train: 0.003 | top1-e-test: 0.392, top5-e-test: 0.153\n",
            "[Epoch  91], lr: 0.00010, Loss: 0.029 | top1-e-train: 0.008, top5-e-train: 0.004 | top1-e-test: 0.391, top5-e-test: 0.151\n",
            "[Epoch  92], lr: 0.00010, Loss: 0.030 | top1-e-train: 0.008, top5-e-train: 0.004 | top1-e-test: 0.392, top5-e-test: 0.151\n",
            "[Epoch  93], lr: 0.00010, Loss: 0.031 | top1-e-train: 0.008, top5-e-train: 0.004 | top1-e-test: 0.391, top5-e-test: 0.153\n",
            "[Epoch  94], lr: 0.00010, Loss: 0.030 | top1-e-train: 0.007, top5-e-train: 0.003 | top1-e-test: 0.391, top5-e-test: 0.152\n",
            "[Epoch  95], lr: 0.00010, Loss: 0.030 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.392, top5-e-test: 0.153\n",
            "[Epoch  96], lr: 0.00010, Loss: 0.030 | top1-e-train: 0.007, top5-e-train: 0.004 | top1-e-test: 0.392, top5-e-test: 0.152\n",
            "[Epoch  97], lr: 0.00010, Loss: 0.031 | top1-e-train: 0.009, top5-e-train: 0.005 | top1-e-test: 0.392, top5-e-test: 0.153\n",
            "[Epoch  98], lr: 0.00010, Loss: 0.029 | top1-e-train: 0.009, top5-e-train: 0.006 | top1-e-test: 0.392, top5-e-test: 0.151\n",
            "[Epoch  99], lr: 0.00010, Loss: 0.030 | top1-e-train: 0.009, top5-e-train: 0.005 | top1-e-test: 0.392, top5-e-test: 0.153\n",
            "[Epoch 100], lr: 0.00010, Loss: 0.030 | top1-e-train: 0.010, top5-e-train: 0.006 | top1-e-test: 0.394, top5-e-test: 0.153\n",
            "--------- Finish Training --------\n",
            "Model:  bam_resnet50_c\n",
            "Top1E:  0.390200018883\n",
            "Top5E:  0.146000027657\n",
            "\n",
            "\n",
            "e1_train:  [0.9751999974250793, 0.9444400072097778, 0.8814600110054016, 0.8209599852561951, 0.7578799724578857, 0.7187399864196777, 0.6720399856567383, 0.6371400356292725, 0.6125999689102173, 0.5604400038719177, 0.5316400527954102, 0.5031000375747681, 0.4701400399208069, 0.4660000205039978, 0.4469200372695923, 0.41912001371383667, 0.3844600319862366, 0.3712199926376343, 0.3501400351524353, 0.32986003160476685, 0.31102001667022705, 0.30511999130249023, 0.2815999984741211, 0.27935999631881714, 0.2927200198173523, 0.23386001586914062, 0.22798001766204834, 0.2095000147819519, 0.19196003675460815, 0.10256004333496094, 0.08520001173019409, 0.07566004991531372, 0.06828004121780396, 0.06586003303527832, 0.055320024490356445, 0.051660001277923584, 0.04880005121231079, 0.0406000018119812, 0.04408001899719238, 0.0408400297164917, 0.03497999906539917, 0.038440048694610596, 0.03137999773025513, 0.026800036430358887, 0.027920007705688477, 0.025220036506652832, 0.029620051383972168, 0.02330005168914795, 0.020660042762756348, 0.02078002691268921, 0.018840014934539795, 0.021720051765441895, 0.016760051250457764, 0.014500021934509277, 0.014240026473999023, 0.013339996337890625, 0.014880001544952393, 0.016279995441436768, 0.014480054378509521, 0.015060007572174072, 0.015520036220550537, 0.01052004098892212, 0.012460052967071533, 0.01166003942489624, 0.010320007801055908, 0.010120034217834473, 0.013180017471313477, 0.010260045528411865, 0.009940028190612793, 0.010500013828277588, 0.006860017776489258, 0.01250004768371582, 0.008180022239685059, 0.009340047836303711, 0.011560022830963135, 0.007940053939819336, 0.011080026626586914, 0.009680032730102539, 0.007760047912597656, 0.013820052146911621, 0.010040044784545898, 0.008420050144195557, 0.008980035781860352, 0.011120021343231201, 0.009640038013458252, 0.010220050811767578, 0.009760022163391113, 0.01016002893447876, 0.01212000846862793, 0.007920026779174805, 0.007600009441375732, 0.007800042629241943, 0.007520020008087158, 0.008240044116973877, 0.007239997386932373, 0.00962001085281372, 0.007019996643066406, 0.008700013160705566, 0.009220004081726074, 0.009420037269592285, 0.009700000286102295]\n",
            "e5_train:  [0.8981199860572815, 0.7805600166320801, 0.6486799716949463, 0.5434200167655945, 0.45732003450393677, 0.39796000719070435, 0.35078001022338867, 0.3165000081062317, 0.2790600061416626, 0.23956000804901123, 0.22080004215240479, 0.19543999433517456, 0.1715800166130066, 0.16898000240325928, 0.15382003784179688, 0.1357000470161438, 0.11480003595352173, 0.11190003156661987, 0.09588003158569336, 0.08667999505996704, 0.07458001375198364, 0.06872004270553589, 0.059020042419433594, 0.05990004539489746, 0.06190001964569092, 0.04232001304626465, 0.04050004482269287, 0.03370004892349243, 0.026960015296936035, 0.01416003704071045, 0.0104600191116333, 0.008540034294128418, 0.007860004901885986, 0.009640038013458252, 0.0056400299072265625, 0.009340047836303711, 0.008620023727416992, 0.004020035266876221, 0.010040044784545898, 0.00868004560470581, 0.006780028343200684, 0.008840024471282959, 0.0042400360107421875, 0.004079997539520264, 0.006780028343200684, 0.005879998207092285, 0.010320007801055908, 0.0042000412940979, 0.004440009593963623, 0.004320025444030762, 0.00466001033782959, 0.006920039653778076, 0.0039400458335876465, 0.0024200081825256348, 0.003080010414123535, 0.002960026264190674, 0.004500031471252441, 0.006720006465911865, 0.005560040473937988, 0.007660031318664551, 0.00866001844406128, 0.003760039806365967, 0.005800008773803711, 0.005700051784515381, 0.003760039806365967, 0.0045400261878967285, 0.006700038909912109, 0.004079997539520264, 0.004420042037963867, 0.00456005334854126, 0.0019200444221496582, 0.007140040397644043, 0.003380000591278076, 0.004420042037963867, 0.006680011749267578, 0.0026800036430358887, 0.006940007209777832, 0.004700005054473877, 0.002980053424835205, 0.008880019187927246, 0.005580008029937744, 0.004220008850097656, 0.004640042781829834, 0.006780028343200684, 0.00532001256942749, 0.006260037422180176, 0.005420029163360596, 0.005760014057159424, 0.008100032806396484, 0.0039000511169433594, 0.003280043601989746, 0.0037000179290771484, 0.003760039806365967, 0.004100024700164795, 0.0025800466537475586, 0.0051400065422058105, 0.0035400390625, 0.004580020904541016, 0.005540013313293457, 0.005380034446716309, 0.005540013313293457]\n",
            "\n",
            "\n",
            "e1_test:  [0.9724000096321106, 0.9451000094413757, 0.8815000057220459, 0.8199000358581543, 0.7440000176429749, 0.7057000398635864, 0.664900004863739, 0.6414999961853027, 0.6205999851226807, 0.5791000127792358, 0.5608999729156494, 0.5412999987602234, 0.5197000503540039, 0.5276000499725342, 0.5134000182151794, 0.49559998512268066, 0.4837000370025635, 0.4796000123023987, 0.47360002994537354, 0.4627000093460083, 0.4538000226020813, 0.4610000252723694, 0.45329999923706055, 0.4506000280380249, 0.4733999967575073, 0.43849998712539673, 0.4456000328063965, 0.44209998846054077, 0.4435000419616699, 0.4043999910354614, 0.4009000062942505, 0.40149998664855957, 0.39649999141693115, 0.39910000562667847, 0.39560002088546753, 0.3950999975204468, 0.3986000418663025, 0.40000003576278687, 0.3985000252723694, 0.3986000418663025, 0.4004000425338745, 0.4004000425338745, 0.39590001106262207, 0.40130001306533813, 0.39719998836517334, 0.4016000032424927, 0.4021000266075134, 0.39649999141693115, 0.39750003814697266, 0.39730000495910645, 0.3961000442504883, 0.39649999141693115, 0.39820003509521484, 0.3985000252723694, 0.40070003271102905, 0.399399995803833, 0.3968999981880188, 0.3955000042915344, 0.397100031375885, 0.39420002698898315, 0.3930000066757202, 0.39340001344680786, 0.39649999141693115, 0.3937000036239624, 0.3912000060081482, 0.39240002632141113, 0.3912000060081482, 0.39160001277923584, 0.3907000422477722, 0.390500009059906, 0.3906000256538391, 0.3928000330924988, 0.3931000232696533, 0.3913000226020813, 0.3927000164985657, 0.39170002937316895, 0.39020001888275146, 0.3937000036239624, 0.3946000337600708, 0.3928000330924988, 0.39430004358291626, 0.3910999894142151, 0.39259999990463257, 0.3928000330924988, 0.3903999924659729, 0.3928999900817871, 0.3931000232696533, 0.39160001277923584, 0.3917999863624573, 0.3937000036239624, 0.39240002632141113, 0.3907000422477722, 0.39160001277923584, 0.3910999894142151, 0.3913000226020813, 0.3917999863624573, 0.3919000029563904, 0.3919000029563904, 0.3921999931335449, 0.3920000195503235, 0.3937000036239624]\n",
            "e5_test:  [0.8982000350952148, 0.775600016117096, 0.6377999782562256, 0.5471000075340271, 0.4524000287055969, 0.399399995803833, 0.3533000349998474, 0.3335000276565552, 0.29809999465942383, 0.26829999685287476, 0.2551000118255615, 0.24080002307891846, 0.22040003538131714, 0.22360002994537354, 0.22150003910064697, 0.20170003175735474, 0.19060003757476807, 0.19710004329681396, 0.18560004234313965, 0.18730002641677856, 0.18029999732971191, 0.18550002574920654, 0.17730003595352173, 0.17980003356933594, 0.18620002269744873, 0.16860002279281616, 0.17080003023147583, 0.16830003261566162, 0.16589999198913574, 0.14650005102157593, 0.14670002460479736, 0.14740002155303955, 0.1470000147819519, 0.14600002765655518, 0.14850002527236938, 0.14990001916885376, 0.1495000123977661, 0.1504000425338745, 0.14930003881454468, 0.15290004014968872, 0.15230000019073486, 0.15210002660751343, 0.14800000190734863, 0.14930003881454468, 0.14960002899169922, 0.1495000123977661, 0.15410000085830688, 0.15170001983642578, 0.15380001068115234, 0.15540003776550293, 0.15280002355575562, 0.15479999780654907, 0.15380001068115234, 0.15320003032684326, 0.15200001001358032, 0.15240001678466797, 0.15140002965927124, 0.15130001306533813, 0.1527000069618225, 0.1527000069618225, 0.15130001306533813, 0.1511000394821167, 0.1510000228881836, 0.15060001611709595, 0.1510000228881836, 0.15260004997253418, 0.15230000019073486, 0.1511000394821167, 0.1518000364303589, 0.1511000394821167, 0.1509000062942505, 0.15200001001358032, 0.15200001001358032, 0.15260004997253418, 0.1535000205039978, 0.15240001678466797, 0.15310001373291016, 0.1518000364303589, 0.15410000085830688, 0.15500003099441528, 0.15230000019073486, 0.15189999341964722, 0.15200001001358032, 0.15140002965927124, 0.15369999408721924, 0.1536000370979309, 0.1527000069618225, 0.15200001001358032, 0.15160000324249268, 0.15130001306533813, 0.15330004692077637, 0.15130001306533813, 0.15130001306533813, 0.15280002355575562, 0.15200001001358032, 0.15260004997253418, 0.15210002660751343, 0.15280002355575562, 0.15130001306533813, 0.15310001373291016, 0.15320003032684326]\n",
            "\n",
            "\n",
            "TIME:  19362.3505659\n",
            "--------------- DONE!!! ----------------------------\n",
            "\n",
            "\n",
            "Namespace(arch='bam_resnet34_c', batch_size=128, epoch=100, learning_rate=0.1)\n",
            "Model:  bam_resnet34_c\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[Epoch   0], lr: 0.10000, Loss: 4.253 | top1-e-train: 0.898, top5-e-train: 0.674 | top1-e-test: 0.894, top5-e-test: 0.658\n",
            "[Epoch   1], lr: 0.10000, Loss: 3.500 | top1-e-train: 0.794, top5-e-train: 0.501 | top1-e-test: 0.791, top5-e-test: 0.497\n",
            "[Epoch   2], lr: 0.10000, Loss: 3.005 | top1-e-train: 0.732, top5-e-train: 0.418 | top1-e-test: 0.726, top5-e-test: 0.410\n",
            "[Epoch   3], lr: 0.10000, Loss: 2.667 | top1-e-train: 0.671, top5-e-train: 0.351 | top1-e-test: 0.672, top5-e-test: 0.350\n",
            "[Epoch   4], lr: 0.10000, Loss: 2.414 | top1-e-train: 0.634, top5-e-train: 0.306 | top1-e-test: 0.638, top5-e-test: 0.317\n",
            "[Epoch   5], lr: 0.10000, Loss: 2.220 | top1-e-train: 0.576, top5-e-train: 0.248 | top1-e-test: 0.590, top5-e-test: 0.273\n",
            "[Epoch   6], lr: 0.10000, Loss: 2.051 | top1-e-train: 0.539, top5-e-train: 0.218 | top1-e-test: 0.564, top5-e-test: 0.251\n",
            "[Epoch   7], lr: 0.10000, Loss: 1.954 | top1-e-train: 0.509, top5-e-train: 0.196 | top1-e-test: 0.539, top5-e-test: 0.232\n",
            "[Epoch   8], lr: 0.10000, Loss: 1.847 | top1-e-train: 0.504, top5-e-train: 0.192 | top1-e-test: 0.533, top5-e-test: 0.231\n",
            "[Epoch   9], lr: 0.10000, Loss: 1.735 | top1-e-train: 0.456, top5-e-train: 0.154 | top1-e-test: 0.505, top5-e-test: 0.203\n",
            "[Epoch  10], lr: 0.10000, Loss: 1.624 | top1-e-train: 0.435, top5-e-train: 0.142 | top1-e-test: 0.504, top5-e-test: 0.201\n",
            "[Epoch  11], lr: 0.10000, Loss: 1.526 | top1-e-train: 0.410, top5-e-train: 0.125 | top1-e-test: 0.484, top5-e-test: 0.191\n",
            "[Epoch  12], lr: 0.10000, Loss: 1.434 | top1-e-train: 0.375, top5-e-train: 0.108 | top1-e-test: 0.466, top5-e-test: 0.175\n",
            "[Epoch  13], lr: 0.10000, Loss: 1.345 | top1-e-train: 0.371, top5-e-train: 0.098 | top1-e-test: 0.472, top5-e-test: 0.181\n",
            "[Epoch  14], lr: 0.10000, Loss: 1.281 | top1-e-train: 0.340, top5-e-train: 0.084 | top1-e-test: 0.461, top5-e-test: 0.179\n",
            "[Epoch  15], lr: 0.10000, Loss: 1.243 | top1-e-train: 0.353, top5-e-train: 0.091 | top1-e-test: 0.460, top5-e-test: 0.176\n",
            "[Epoch  16], lr: 0.10000, Loss: 1.264 | top1-e-train: 0.324, top5-e-train: 0.078 | top1-e-test: 0.458, top5-e-test: 0.174\n",
            "[Epoch  17], lr: 0.10000, Loss: 1.096 | top1-e-train: 0.305, top5-e-train: 0.068 | top1-e-test: 0.452, top5-e-test: 0.172\n",
            "[Epoch  18], lr: 0.10000, Loss: 1.013 | top1-e-train: 0.258, top5-e-train: 0.048 | top1-e-test: 0.432, top5-e-test: 0.159\n",
            "[Epoch  19], lr: 0.10000, Loss: 0.935 | top1-e-train: 0.245, top5-e-train: 0.041 | top1-e-test: 0.445, top5-e-test: 0.163\n",
            "[Epoch  20], lr: 0.10000, Loss: 0.877 | top1-e-train: 0.245, top5-e-train: 0.042 | top1-e-test: 0.445, top5-e-test: 0.166\n",
            "[Epoch  21], lr: 0.10000, Loss: 1.046 | top1-e-train: 0.248, top5-e-train: 0.047 | top1-e-test: 0.453, top5-e-test: 0.184\n",
            "[Epoch  22], lr: 0.10000, Loss: 0.819 | top1-e-train: 0.206, top5-e-train: 0.031 | top1-e-test: 0.444, top5-e-test: 0.168\n",
            "[Epoch  23], lr: 0.10000, Loss: 0.719 | top1-e-train: 0.172, top5-e-train: 0.020 | top1-e-test: 0.433, top5-e-test: 0.163\n",
            "[Epoch  24], lr: 0.10000, Loss: 0.676 | top1-e-train: 0.176, top5-e-train: 0.020 | top1-e-test: 0.430, top5-e-test: 0.164\n",
            "[Epoch  25], lr: 0.10000, Loss: 0.611 | top1-e-train: 0.172, top5-e-train: 0.018 | top1-e-test: 0.432, top5-e-test: 0.166\n",
            "[Epoch  26], lr: 0.10000, Loss: 0.568 | top1-e-train: 0.146, top5-e-train: 0.014 | top1-e-test: 0.427, top5-e-test: 0.161\n",
            "[Epoch  27], lr: 0.10000, Loss: 0.501 | top1-e-train: 0.122, top5-e-train: 0.009 | top1-e-test: 0.423, top5-e-test: 0.158\n",
            "[Epoch  28], lr: 0.10000, Loss: 0.467 | top1-e-train: 0.112, top5-e-train: 0.008 | top1-e-test: 0.422, top5-e-test: 0.160\n",
            "[Epoch  29], lr: 0.01000, Loss: 0.249 | top1-e-train: 0.046, top5-e-train: 0.003 | top1-e-test: 0.391, top5-e-test: 0.144\n",
            "[Epoch  30], lr: 0.01000, Loss: 0.185 | top1-e-train: 0.035, top5-e-train: 0.002 | top1-e-test: 0.390, top5-e-test: 0.144\n",
            "[Epoch  31], lr: 0.01000, Loss: 0.149 | top1-e-train: 0.030, top5-e-train: 0.002 | top1-e-test: 0.387, top5-e-test: 0.143\n",
            "[Epoch  32], lr: 0.01000, Loss: 0.136 | top1-e-train: 0.026, top5-e-train: 0.002 | top1-e-test: 0.386, top5-e-test: 0.144\n",
            "[Epoch  33], lr: 0.01000, Loss: 0.121 | top1-e-train: 0.026, top5-e-train: 0.002 | top1-e-test: 0.390, top5-e-test: 0.144\n",
            "[Epoch  34], lr: 0.01000, Loss: 0.113 | top1-e-train: 0.021, top5-e-train: 0.002 | top1-e-test: 0.389, top5-e-test: 0.145\n",
            "[Epoch  35], lr: 0.01000, Loss: 0.106 | top1-e-train: 0.021, top5-e-train: 0.003 | top1-e-test: 0.393, top5-e-test: 0.148\n",
            "[Epoch  36], lr: 0.01000, Loss: 0.097 | top1-e-train: 0.019, top5-e-train: 0.002 | top1-e-test: 0.391, top5-e-test: 0.147\n",
            "[Epoch  37], lr: 0.01000, Loss: 0.093 | top1-e-train: 0.017, top5-e-train: 0.002 | top1-e-test: 0.389, top5-e-test: 0.146\n",
            "[Epoch  38], lr: 0.01000, Loss: 0.089 | top1-e-train: 0.017, top5-e-train: 0.002 | top1-e-test: 0.390, top5-e-test: 0.147\n",
            "[Epoch  39], lr: 0.01000, Loss: 0.083 | top1-e-train: 0.015, top5-e-train: 0.002 | top1-e-test: 0.391, top5-e-test: 0.146\n",
            "[Epoch  40], lr: 0.01000, Loss: 0.081 | top1-e-train: 0.016, top5-e-train: 0.003 | top1-e-test: 0.392, top5-e-test: 0.152\n",
            "[Epoch  41], lr: 0.01000, Loss: 0.076 | top1-e-train: 0.014, top5-e-train: 0.005 | top1-e-test: 0.390, top5-e-test: 0.148\n",
            "[Epoch  42], lr: 0.01000, Loss: 0.071 | top1-e-train: 0.015, top5-e-train: 0.007 | top1-e-test: 0.393, top5-e-test: 0.152\n",
            "[Epoch  43], lr: 0.01000, Loss: 0.068 | top1-e-train: 0.013, top5-e-train: 0.005 | top1-e-test: 0.391, top5-e-test: 0.151\n",
            "[Epoch  44], lr: 0.01000, Loss: 0.067 | top1-e-train: 0.014, top5-e-train: 0.006 | top1-e-test: 0.393, top5-e-test: 0.152\n",
            "[Epoch  45], lr: 0.01000, Loss: 0.061 | top1-e-train: 0.014, top5-e-train: 0.006 | top1-e-test: 0.393, top5-e-test: 0.154\n",
            "[Epoch  46], lr: 0.01000, Loss: 0.058 | top1-e-train: 0.014, top5-e-train: 0.007 | top1-e-test: 0.392, top5-e-test: 0.155\n",
            "[Epoch  47], lr: 0.01000, Loss: 0.055 | top1-e-train: 0.011, top5-e-train: 0.005 | top1-e-test: 0.391, top5-e-test: 0.149\n",
            "[Epoch  48], lr: 0.01000, Loss: 0.051 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.390, top5-e-test: 0.152\n",
            "[Epoch  49], lr: 0.01000, Loss: 0.051 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.390, top5-e-test: 0.153\n",
            "[Epoch  50], lr: 0.01000, Loss: 0.047 | top1-e-train: 0.009, top5-e-train: 0.004 | top1-e-test: 0.387, top5-e-test: 0.152\n",
            "[Epoch  51], lr: 0.01000, Loss: 0.045 | top1-e-train: 0.013, top5-e-train: 0.007 | top1-e-test: 0.396, top5-e-test: 0.156\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}