{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "run_all_s1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyenvietan/DL_Attention_PA1_ee898/blob/master/run_all_s1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dHe2BCurrR9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3296
        },
        "outputId": "a4e93a04-3c9e-4adb-8b48-93ef63a532fc"
      },
      "source": [
        "!bash run_all_s1.sh\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(arch='bam_resnet50_s', batch_size=128, epoch=100, learning_rate=0.1)\n",
            "Model:  bam_resnet50_s\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[Epoch   0], lr: 0.10000, Loss: 4.912 | top1-e-train: 0.988, top5-e-train: 0.940 | top1-e-test: 0.987, top5-e-test: 0.943\n",
            "[Epoch   1], lr: 0.10000, Loss: 4.563 | top1-e-train: 0.977, top5-e-train: 0.889 | top1-e-test: 0.976, top5-e-test: 0.890\n",
            "[Epoch   2], lr: 0.10000, Loss: 4.328 | top1-e-train: 0.958, top5-e-train: 0.822 | top1-e-test: 0.956, top5-e-test: 0.815\n",
            "[Epoch   3], lr: 0.10000, Loss: 4.066 | top1-e-train: 0.924, top5-e-train: 0.722 | top1-e-test: 0.925, top5-e-test: 0.719\n",
            "[Epoch   4], lr: 0.10000, Loss: 3.867 | top1-e-train: 0.898, top5-e-train: 0.655 | top1-e-test: 0.893, top5-e-test: 0.640\n",
            "[Epoch   5], lr: 0.10000, Loss: 3.661 | top1-e-train: 0.863, top5-e-train: 0.597 | top1-e-test: 0.857, top5-e-test: 0.578\n",
            "[Epoch   6], lr: 0.10000, Loss: 3.443 | top1-e-train: 0.806, top5-e-train: 0.516 | top1-e-test: 0.804, top5-e-test: 0.509\n",
            "[Epoch   7], lr: 0.10000, Loss: 3.246 | top1-e-train: 0.778, top5-e-train: 0.477 | top1-e-test: 0.770, top5-e-test: 0.470\n",
            "[Epoch   8], lr: 0.10000, Loss: 3.069 | top1-e-train: 0.741, top5-e-train: 0.424 | top1-e-test: 0.740, top5-e-test: 0.430\n",
            "[Epoch   9], lr: 0.10000, Loss: 2.907 | top1-e-train: 0.705, top5-e-train: 0.385 | top1-e-test: 0.703, top5-e-test: 0.388\n",
            "[Epoch  10], lr: 0.10000, Loss: 2.739 | top1-e-train: 0.664, top5-e-train: 0.339 | top1-e-test: 0.665, top5-e-test: 0.350\n",
            "[Epoch  11], lr: 0.10000, Loss: 2.585 | top1-e-train: 0.633, top5-e-train: 0.302 | top1-e-test: 0.644, top5-e-test: 0.320\n",
            "[Epoch  12], lr: 0.10000, Loss: 2.442 | top1-e-train: 0.624, top5-e-train: 0.289 | top1-e-test: 0.629, top5-e-test: 0.303\n",
            "[Epoch  13], lr: 0.10000, Loss: 2.322 | top1-e-train: 0.587, top5-e-train: 0.260 | top1-e-test: 0.608, top5-e-test: 0.282\n",
            "[Epoch  14], lr: 0.10000, Loss: 2.211 | top1-e-train: 0.562, top5-e-train: 0.238 | top1-e-test: 0.585, top5-e-test: 0.273\n",
            "[Epoch  15], lr: 0.10000, Loss: 2.115 | top1-e-train: 0.535, top5-e-train: 0.219 | top1-e-test: 0.568, top5-e-test: 0.253\n",
            "[Epoch  16], lr: 0.10000, Loss: 2.022 | top1-e-train: 0.512, top5-e-train: 0.200 | top1-e-test: 0.554, top5-e-test: 0.243\n",
            "[Epoch  17], lr: 0.10000, Loss: 1.928 | top1-e-train: 0.485, top5-e-train: 0.179 | top1-e-test: 0.534, top5-e-test: 0.226\n",
            "[Epoch  18], lr: 0.10000, Loss: 1.838 | top1-e-train: 0.484, top5-e-train: 0.178 | top1-e-test: 0.545, top5-e-test: 0.227\n",
            "[Epoch  19], lr: 0.10000, Loss: 1.800 | top1-e-train: 0.454, top5-e-train: 0.158 | top1-e-test: 0.522, top5-e-test: 0.215\n",
            "[Epoch  20], lr: 0.10000, Loss: 1.692 | top1-e-train: 0.430, top5-e-train: 0.144 | top1-e-test: 0.515, top5-e-test: 0.214\n",
            "[Epoch  21], lr: 0.10000, Loss: 1.610 | top1-e-train: 0.455, top5-e-train: 0.156 | top1-e-test: 0.524, top5-e-test: 0.225\n",
            "[Epoch  22], lr: 0.10000, Loss: 1.547 | top1-e-train: 0.398, top5-e-train: 0.121 | top1-e-test: 0.493, top5-e-test: 0.200\n",
            "[Epoch  23], lr: 0.10000, Loss: 1.479 | top1-e-train: 0.379, top5-e-train: 0.108 | top1-e-test: 0.488, top5-e-test: 0.195\n",
            "[Epoch  24], lr: 0.10000, Loss: 1.405 | top1-e-train: 0.363, top5-e-train: 0.101 | top1-e-test: 0.488, top5-e-test: 0.192\n",
            "[Epoch  25], lr: 0.10000, Loss: 1.332 | top1-e-train: 0.356, top5-e-train: 0.097 | top1-e-test: 0.480, top5-e-test: 0.192\n",
            "[Epoch  26], lr: 0.10000, Loss: 1.272 | top1-e-train: 0.327, top5-e-train: 0.083 | top1-e-test: 0.471, top5-e-test: 0.186\n",
            "[Epoch  27], lr: 0.10000, Loss: 1.204 | top1-e-train: 0.319, top5-e-train: 0.075 | top1-e-test: 0.478, top5-e-test: 0.190\n",
            "[Epoch  28], lr: 0.10000, Loss: 1.144 | top1-e-train: 0.398, top5-e-train: 0.117 | top1-e-test: 0.538, top5-e-test: 0.234\n",
            "[Epoch  29], lr: 0.01000, Loss: 0.804 | top1-e-train: 0.197, top5-e-train: 0.035 | top1-e-test: 0.431, top5-e-test: 0.158\n",
            "[Epoch  30], lr: 0.01000, Loss: 0.693 | top1-e-train: 0.177, top5-e-train: 0.030 | top1-e-test: 0.427, top5-e-test: 0.159\n",
            "[Epoch  31], lr: 0.01000, Loss: 0.632 | top1-e-train: 0.161, top5-e-train: 0.025 | top1-e-test: 0.424, top5-e-test: 0.159\n",
            "[Epoch  32], lr: 0.01000, Loss: 0.596 | top1-e-train: 0.152, top5-e-train: 0.022 | top1-e-test: 0.425, top5-e-test: 0.159\n",
            "[Epoch  33], lr: 0.01000, Loss: 0.558 | top1-e-train: 0.142, top5-e-train: 0.021 | top1-e-test: 0.428, top5-e-test: 0.162\n",
            "[Epoch  34], lr: 0.01000, Loss: 0.520 | top1-e-train: 0.130, top5-e-train: 0.018 | top1-e-test: 0.426, top5-e-test: 0.157\n",
            "[Epoch  35], lr: 0.01000, Loss: 0.489 | top1-e-train: 0.124, top5-e-train: 0.016 | top1-e-test: 0.428, top5-e-test: 0.165\n",
            "[Epoch  36], lr: 0.01000, Loss: 0.462 | top1-e-train: 0.113, top5-e-train: 0.014 | top1-e-test: 0.430, top5-e-test: 0.164\n",
            "[Epoch  37], lr: 0.01000, Loss: 0.436 | top1-e-train: 0.103, top5-e-train: 0.012 | top1-e-test: 0.427, top5-e-test: 0.164\n",
            "[Epoch  38], lr: 0.01000, Loss: 0.404 | top1-e-train: 0.095, top5-e-train: 0.011 | top1-e-test: 0.430, top5-e-test: 0.167\n",
            "[Epoch  39], lr: 0.01000, Loss: 0.382 | top1-e-train: 0.084, top5-e-train: 0.008 | top1-e-test: 0.428, top5-e-test: 0.167\n",
            "[Epoch  40], lr: 0.01000, Loss: 0.352 | top1-e-train: 0.085, top5-e-train: 0.009 | top1-e-test: 0.433, top5-e-test: 0.172\n",
            "[Epoch  41], lr: 0.01000, Loss: 0.336 | top1-e-train: 0.072, top5-e-train: 0.009 | top1-e-test: 0.435, top5-e-test: 0.170\n",
            "[Epoch  42], lr: 0.01000, Loss: 0.310 | top1-e-train: 0.067, top5-e-train: 0.006 | top1-e-test: 0.432, top5-e-test: 0.167\n",
            "[Epoch  43], lr: 0.01000, Loss: 0.291 | top1-e-train: 0.063, top5-e-train: 0.007 | top1-e-test: 0.434, top5-e-test: 0.170\n",
            "[Epoch  44], lr: 0.01000, Loss: 0.270 | top1-e-train: 0.056, top5-e-train: 0.006 | top1-e-test: 0.433, top5-e-test: 0.170\n",
            "[Epoch  45], lr: 0.01000, Loss: 0.255 | top1-e-train: 0.055, top5-e-train: 0.006 | top1-e-test: 0.432, top5-e-test: 0.173\n",
            "[Epoch  46], lr: 0.01000, Loss: 0.239 | top1-e-train: 0.048, top5-e-train: 0.006 | top1-e-test: 0.435, top5-e-test: 0.168\n",
            "[Epoch  47], lr: 0.01000, Loss: 0.221 | top1-e-train: 0.045, top5-e-train: 0.005 | top1-e-test: 0.433, top5-e-test: 0.172\n",
            "[Epoch  48], lr: 0.01000, Loss: 0.207 | top1-e-train: 0.040, top5-e-train: 0.003 | top1-e-test: 0.432, top5-e-test: 0.170\n",
            "[Epoch  49], lr: 0.01000, Loss: 0.198 | top1-e-train: 0.040, top5-e-train: 0.006 | top1-e-test: 0.432, top5-e-test: 0.173\n",
            "[Epoch  50], lr: 0.01000, Loss: 0.182 | top1-e-train: 0.033, top5-e-train: 0.003 | top1-e-test: 0.431, top5-e-test: 0.170\n",
            "[Epoch  51], lr: 0.01000, Loss: 0.180 | top1-e-train: 0.032, top5-e-train: 0.003 | top1-e-test: 0.433, top5-e-test: 0.171\n",
            "[Epoch  52], lr: 0.01000, Loss: 0.166 | top1-e-train: 0.033, top5-e-train: 0.005 | top1-e-test: 0.431, top5-e-test: 0.172\n",
            "[Epoch  53], lr: 0.01000, Loss: 0.151 | top1-e-train: 0.034, top5-e-train: 0.006 | top1-e-test: 0.436, top5-e-test: 0.173\n",
            "[Epoch  54], lr: 0.01000, Loss: 0.148 | top1-e-train: 0.032, top5-e-train: 0.005 | top1-e-test: 0.434, top5-e-test: 0.177\n",
            "[Epoch  55], lr: 0.01000, Loss: 0.138 | top1-e-train: 0.025, top5-e-train: 0.002 | top1-e-test: 0.429, top5-e-test: 0.170\n",
            "[Epoch  56], lr: 0.01000, Loss: 0.134 | top1-e-train: 0.023, top5-e-train: 0.001 | top1-e-test: 0.428, top5-e-test: 0.171\n",
            "[Epoch  57], lr: 0.01000, Loss: 0.125 | top1-e-train: 0.023, top5-e-train: 0.003 | top1-e-test: 0.433, top5-e-test: 0.174\n",
            "[Epoch  58], lr: 0.01000, Loss: 0.117 | top1-e-train: 0.023, top5-e-train: 0.002 | top1-e-test: 0.433, top5-e-test: 0.175\n",
            "[Epoch  59], lr: 0.00100, Loss: 0.095 | top1-e-train: 0.019, top5-e-train: 0.003 | top1-e-test: 0.428, top5-e-test: 0.170\n",
            "[Epoch  60], lr: 0.00100, Loss: 0.085 | top1-e-train: 0.015, top5-e-train: 0.001 | top1-e-test: 0.424, top5-e-test: 0.169\n",
            "[Epoch  61], lr: 0.00100, Loss: 0.077 | top1-e-train: 0.016, top5-e-train: 0.002 | top1-e-test: 0.425, top5-e-test: 0.172\n",
            "[Epoch  62], lr: 0.00100, Loss: 0.072 | top1-e-train: 0.013, top5-e-train: 0.001 | top1-e-test: 0.425, top5-e-test: 0.170\n",
            "[Epoch  63], lr: 0.00100, Loss: 0.073 | top1-e-train: 0.017, top5-e-train: 0.003 | top1-e-test: 0.429, top5-e-test: 0.170\n",
            "[Epoch  64], lr: 0.00100, Loss: 0.072 | top1-e-train: 0.017, top5-e-train: 0.004 | top1-e-test: 0.424, top5-e-test: 0.171\n",
            "[Epoch  65], lr: 0.00100, Loss: 0.071 | top1-e-train: 0.013, top5-e-train: 0.002 | top1-e-test: 0.425, top5-e-test: 0.171\n",
            "[Epoch  66], lr: 0.00100, Loss: 0.068 | top1-e-train: 0.015, top5-e-train: 0.003 | top1-e-test: 0.426, top5-e-test: 0.170\n",
            "[Epoch  67], lr: 0.00100, Loss: 0.067 | top1-e-train: 0.013, top5-e-train: 0.002 | top1-e-test: 0.425, top5-e-test: 0.168\n",
            "[Epoch  68], lr: 0.00100, Loss: 0.065 | top1-e-train: 0.011, top5-e-train: 0.002 | top1-e-test: 0.424, top5-e-test: 0.169\n",
            "[Epoch  69], lr: 0.00100, Loss: 0.063 | top1-e-train: 0.017, top5-e-train: 0.005 | top1-e-test: 0.429, top5-e-test: 0.170\n",
            "[Epoch  70], lr: 0.00100, Loss: 0.062 | top1-e-train: 0.014, top5-e-train: 0.003 | top1-e-test: 0.426, top5-e-test: 0.170\n",
            "[Epoch  71], lr: 0.00100, Loss: 0.062 | top1-e-train: 0.011, top5-e-train: 0.002 | top1-e-test: 0.423, top5-e-test: 0.167\n",
            "[Epoch  72], lr: 0.00100, Loss: 0.064 | top1-e-train: 0.012, top5-e-train: 0.003 | top1-e-test: 0.425, top5-e-test: 0.169\n",
            "[Epoch  73], lr: 0.00100, Loss: 0.060 | top1-e-train: 0.013, top5-e-train: 0.005 | top1-e-test: 0.424, top5-e-test: 0.172\n",
            "[Epoch  74], lr: 0.00100, Loss: 0.060 | top1-e-train: 0.011, top5-e-train: 0.004 | top1-e-test: 0.427, top5-e-test: 0.170\n",
            "[Epoch  75], lr: 0.00100, Loss: 0.059 | top1-e-train: 0.011, top5-e-train: 0.003 | top1-e-test: 0.425, top5-e-test: 0.170\n",
            "[Epoch  76], lr: 0.00100, Loss: 0.061 | top1-e-train: 0.012, top5-e-train: 0.004 | top1-e-test: 0.425, top5-e-test: 0.172\n",
            "[Epoch  77], lr: 0.00100, Loss: 0.057 | top1-e-train: 0.013, top5-e-train: 0.005 | top1-e-test: 0.426, top5-e-test: 0.172\n",
            "[Epoch  78], lr: 0.00100, Loss: 0.055 | top1-e-train: 0.010, top5-e-train: 0.004 | top1-e-test: 0.424, top5-e-test: 0.172\n",
            "[Epoch  79], lr: 0.00100, Loss: 0.056 | top1-e-train: 0.013, top5-e-train: 0.005 | top1-e-test: 0.427, top5-e-test: 0.172\n",
            "[Epoch  80], lr: 0.00100, Loss: 0.055 | top1-e-train: 0.012, top5-e-train: 0.004 | top1-e-test: 0.425, top5-e-test: 0.171\n",
            "[Epoch  81], lr: 0.00100, Loss: 0.055 | top1-e-train: 0.013, top5-e-train: 0.005 | top1-e-test: 0.426, top5-e-test: 0.173\n",
            "[Epoch  82], lr: 0.00100, Loss: 0.054 | top1-e-train: 0.010, top5-e-train: 0.004 | top1-e-test: 0.425, top5-e-test: 0.172\n",
            "[Epoch  83], lr: 0.00100, Loss: 0.053 | top1-e-train: 0.010, top5-e-train: 0.003 | top1-e-test: 0.425, top5-e-test: 0.171\n",
            "[Epoch  84], lr: 0.00100, Loss: 0.053 | top1-e-train: 0.012, top5-e-train: 0.005 | top1-e-test: 0.428, top5-e-test: 0.174\n",
            "[Epoch  85], lr: 0.00100, Loss: 0.051 | top1-e-train: 0.011, top5-e-train: 0.005 | top1-e-test: 0.423, top5-e-test: 0.174\n",
            "[Epoch  86], lr: 0.00100, Loss: 0.052 | top1-e-train: 0.011, top5-e-train: 0.005 | top1-e-test: 0.424, top5-e-test: 0.172\n",
            "[Epoch  87], lr: 0.00100, Loss: 0.051 | top1-e-train: 0.012, top5-e-train: 0.005 | top1-e-test: 0.426, top5-e-test: 0.173\n",
            "[Epoch  88], lr: 0.00100, Loss: 0.052 | top1-e-train: 0.013, top5-e-train: 0.006 | top1-e-test: 0.426, top5-e-test: 0.173\n",
            "[Epoch  89], lr: 0.00010, Loss: 0.048 | top1-e-train: 0.009, top5-e-train: 0.003 | top1-e-test: 0.424, top5-e-test: 0.169\n",
            "[Epoch  90], lr: 0.00010, Loss: 0.050 | top1-e-train: 0.013, top5-e-train: 0.007 | top1-e-test: 0.426, top5-e-test: 0.174\n",
            "[Epoch  91], lr: 0.00010, Loss: 0.047 | top1-e-train: 0.011, top5-e-train: 0.005 | top1-e-test: 0.425, top5-e-test: 0.172\n",
            "[Epoch  92], lr: 0.00010, Loss: 0.047 | top1-e-train: 0.011, top5-e-train: 0.006 | top1-e-test: 0.424, top5-e-test: 0.172\n",
            "[Epoch  93], lr: 0.00010, Loss: 0.048 | top1-e-train: 0.010, top5-e-train: 0.004 | top1-e-test: 0.423, top5-e-test: 0.171\n",
            "[Epoch  94], lr: 0.00010, Loss: 0.048 | top1-e-train: 0.010, top5-e-train: 0.004 | top1-e-test: 0.423, top5-e-test: 0.170\n",
            "[Epoch  95], lr: 0.00010, Loss: 0.046 | top1-e-train: 0.010, top5-e-train: 0.004 | top1-e-test: 0.424, top5-e-test: 0.170\n",
            "[Epoch  96], lr: 0.00010, Loss: 0.049 | top1-e-train: 0.009, top5-e-train: 0.003 | top1-e-test: 0.423, top5-e-test: 0.171\n",
            "[Epoch  97], lr: 0.00010, Loss: 0.050 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.424, top5-e-test: 0.169\n",
            "[Epoch  98], lr: 0.00010, Loss: 0.050 | top1-e-train: 0.012, top5-e-train: 0.005 | top1-e-test: 0.425, top5-e-test: 0.171\n",
            "[Epoch  99], lr: 0.00010, Loss: 0.047 | top1-e-train: 0.010, top5-e-train: 0.005 | top1-e-test: 0.423, top5-e-test: 0.170\n",
            "[Epoch 100], lr: 0.00010, Loss: 0.047 | top1-e-train: 0.012, top5-e-train: 0.007 | top1-e-test: 0.426, top5-e-test: 0.174\n",
            "--------- Finish Training --------\n",
            "Model:  bam_resnet50_s\n",
            "Top1E:  0.422699987888\n",
            "Top5E:  0.157299995422\n",
            "\n",
            "\n",
            "e1_train:  [0.9879599809646606, 0.976580023765564, 0.9581599831581116, 0.9237200021743774, 0.8982399702072144, 0.8632799983024597, 0.8060600161552429, 0.778219997882843, 0.7406799793243408, 0.7048599720001221, 0.6638200283050537, 0.63264000415802, 0.6244800090789795, 0.5870000123977661, 0.561680018901825, 0.5354800224304199, 0.5119800567626953, 0.4848400354385376, 0.4839800000190735, 0.45412003993988037, 0.4295400381088257, 0.45490002632141113, 0.3978400230407715, 0.37860000133514404, 0.3630800247192383, 0.3557800054550171, 0.3270600438117981, 0.31926000118255615, 0.3983800411224365, 0.19701999425888062, 0.17694002389907837, 0.160800039768219, 0.1522200107574463, 0.1420000195503235, 0.12994003295898438, 0.12418001890182495, 0.11278003454208374, 0.10302001237869263, 0.09518003463745117, 0.08400005102157593, 0.08508002758026123, 0.07244002819061279, 0.06718003749847412, 0.06319999694824219, 0.05642002820968628, 0.054680049419403076, 0.04838001728057861, 0.045340001583099365, 0.03970003128051758, 0.040040016174316406, 0.03320002555847168, 0.03242003917694092, 0.03336000442504883, 0.03431999683380127, 0.032280027866363525, 0.0252000093460083, 0.022960007190704346, 0.023320019245147705, 0.02303999662399292, 0.018880009651184082, 0.014860033988952637, 0.015720009803771973, 0.013100028038024902, 0.01714003086090088, 0.016560018062591553, 0.013000011444091797, 0.014900028705596924, 0.013040006160736084, 0.01146000623703003, 0.016660034656524658, 0.014260053634643555, 0.011440038681030273, 0.01212000846862793, 0.013040006160736084, 0.011160016059875488, 0.010980010032653809, 0.011960029602050781, 0.012920022010803223, 0.01034003496170044, 0.01250004768371582, 0.012240052223205566, 0.01278001070022583, 0.010200023651123047, 0.010140001773834229, 0.011620044708251953, 0.010600030422210693, 0.011400043964385986, 0.012360036373138428, 0.012560009956359863, 0.00850003957748413, 0.013080000877380371, 0.01100003719329834, 0.011420011520385742, 0.009700000286102295, 0.009540021419525146, 0.009540021419525146, 0.008520007133483887, 0.010040044784545898, 0.011539995670318604, 0.010399997234344482, 0.012220025062561035]\n",
            "e5_train:  [0.9401800036430359, 0.8890600204467773, 0.8223600387573242, 0.7222599983215332, 0.6554399728775024, 0.5965399742126465, 0.5155799984931946, 0.47746002674102783, 0.4241600036621094, 0.3851799964904785, 0.33862000703811646, 0.3021000027656555, 0.2890000343322754, 0.2603800296783447, 0.238319993019104, 0.2192400097846985, 0.20048004388809204, 0.17866003513336182, 0.17768001556396484, 0.15762001276016235, 0.1437000036239624, 0.15588003396987915, 0.12056005001068115, 0.10822004079818726, 0.10074001550674438, 0.09710001945495605, 0.08314001560211182, 0.07530003786087036, 0.11682003736495972, 0.034600019454956055, 0.029720008373260498, 0.02490001916885376, 0.022140026092529297, 0.020759999752044678, 0.018200039863586426, 0.015740036964416504, 0.013700008392333984, 0.012380003929138184, 0.010660052299499512, 0.008320033550262451, 0.009420037269592285, 0.00868004560470581, 0.006380021572113037, 0.006980001926422119, 0.006460011005401611, 0.006360054016113281, 0.005860030651092529, 0.0053400397300720215, 0.003300011157989502, 0.0059400200843811035, 0.0028400421142578125, 0.0027600526809692383, 0.005300045013427734, 0.0064400434494018555, 0.004739999771118164, 0.0017000436782836914, 0.0014600157737731934, 0.003200054168701172, 0.002460002899169922, 0.003020048141479492, 0.0013800263404846191, 0.0023400187492370605, 0.001139998435974121, 0.0034600496292114258, 0.003880023956298828, 0.002360045909881592, 0.002740025520324707, 0.0023800134658813477, 0.0023400187492370605, 0.004500031471252441, 0.0031000375747680664, 0.0017600059509277344, 0.0031800270080566406, 0.004980027675628662, 0.003800034523010254, 0.0033600330352783203, 0.004060029983520508, 0.004700005054473877, 0.003980040550231934, 0.004920005798339844, 0.004400014877319336, 0.005160033702850342, 0.0039400458335876465, 0.003320038318634033, 0.00532001256942749, 0.005219995975494385, 0.005380034446716309, 0.005300045013427734, 0.006380021572113037, 0.0034800171852111816, 0.006960034370422363, 0.0053400397300720215, 0.005520045757293701, 0.004440009593963623, 0.004400014877319336, 0.004360020160675049, 0.003240048885345459, 0.004920005798339844, 0.0054000020027160645, 0.004920005798339844, 0.006860017776489258]\n",
            "\n",
            "\n",
            "e1_test:  [0.9871000051498413, 0.9763000011444092, 0.9560999870300293, 0.9250999689102173, 0.8931999802589417, 0.8569999933242798, 0.8041000366210938, 0.7700999975204468, 0.7401000261306763, 0.7027000188827515, 0.664900004863739, 0.6438000202178955, 0.6288000345230103, 0.608299970626831, 0.5849000215530396, 0.5677000284194946, 0.5541000366210938, 0.5342999696731567, 0.5450000166893005, 0.5217000246047974, 0.5152000188827515, 0.523900032043457, 0.49330002069473267, 0.48809999227523804, 0.48809999227523804, 0.4796000123023987, 0.4708000421524048, 0.4781000018119812, 0.5382000207901001, 0.4309999942779541, 0.42669999599456787, 0.42410004138946533, 0.4251999855041504, 0.42840003967285156, 0.42580002546310425, 0.42809998989105225, 0.4296000003814697, 0.42669999599456787, 0.42980003356933594, 0.4284999966621399, 0.4326000213623047, 0.43540000915527344, 0.4319000244140625, 0.4336000084877014, 0.43309998512268066, 0.43160003423690796, 0.4350000023841858, 0.4334999918937683, 0.43230003118515015, 0.4323999881744385, 0.43070000410079956, 0.4326000213623047, 0.43090003728866577, 0.43630003929138184, 0.43400001525878906, 0.4291999936103821, 0.42809998989105225, 0.4327000379562378, 0.43320000171661377, 0.42750000953674316, 0.42419999837875366, 0.42500001192092896, 0.4254000186920166, 0.42879998683929443, 0.42419999837875366, 0.42500001192092896, 0.4255000352859497, 0.4254000186920166, 0.4244999885559082, 0.42879998683929443, 0.4261000156402588, 0.4229000210762024, 0.42500001192092896, 0.42410004138946533, 0.42660003900527954, 0.4251999855041504, 0.4251999855041504, 0.42590004205703735, 0.42419999837875366, 0.426800012588501, 0.42489999532699585, 0.4262000322341919, 0.42500001192092896, 0.42489999532699585, 0.4277000427246094, 0.42339998483657837, 0.42430001497268677, 0.4262000322341919, 0.4262000322341919, 0.423799991607666, 0.42640000581741333, 0.42510002851486206, 0.42419999837875366, 0.42309999465942383, 0.42339998483657837, 0.4235000014305115, 0.4226999878883362, 0.4239000082015991, 0.4251999855041504, 0.42309999465942383, 0.4262000322341919]\n",
            "e5_test:  [0.9426000118255615, 0.8901000022888184, 0.8151999711990356, 0.7192000150680542, 0.6401000022888184, 0.5780999660491943, 0.5092999935150146, 0.47040003538131714, 0.42970001697540283, 0.3881000280380249, 0.34970003366470337, 0.3203999996185303, 0.30260002613067627, 0.2824000120162964, 0.2734000086784363, 0.25300002098083496, 0.24300003051757812, 0.2257000207901001, 0.22720003128051758, 0.21490001678466797, 0.2135000228881836, 0.2247999906539917, 0.20020002126693726, 0.19460004568099976, 0.1924000382423401, 0.1915000081062317, 0.18620002269744873, 0.19020003080368042, 0.2337999939918518, 0.15820002555847168, 0.15940004587173462, 0.15880000591278076, 0.1592000126838684, 0.16200000047683716, 0.15729999542236328, 0.16540002822875977, 0.16409999132156372, 0.16360002756118774, 0.1666000485420227, 0.16690003871917725, 0.17150002717971802, 0.16990000009536743, 0.16690003871917725, 0.17010003328323364, 0.17010003328323364, 0.1728000044822693, 0.1680999994277954, 0.17150002717971802, 0.17000001668930054, 0.17320001125335693, 0.1698000431060791, 0.17090004682540894, 0.1722000241279602, 0.17270004749298096, 0.1770000457763672, 0.17020004987716675, 0.17120003700256348, 0.17350000143051147, 0.1746000051498413, 0.17000001668930054, 0.16940003633499146, 0.17200005054473877, 0.17010003328323364, 0.169700026512146, 0.1712999939918518, 0.17110002040863037, 0.17040002346038818, 0.16830003261566162, 0.1690000295639038, 0.17010003328323364, 0.1698000431060791, 0.16699999570846558, 0.1689000129699707, 0.1722000241279602, 0.16990000009536743, 0.17020004987716675, 0.1723000407218933, 0.1722000241279602, 0.1723000407218933, 0.1723000407218933, 0.17080003023147583, 0.17330002784729004, 0.1723000407218933, 0.17100000381469727, 0.17400002479553223, 0.17360001802444458, 0.17160004377365112, 0.17270004749298096, 0.17340004444122314, 0.1689000129699707, 0.17419999837875366, 0.17190003395080566, 0.1721000075340271, 0.17059999704360962, 0.17020004987716675, 0.17040002346038818, 0.1705000400543213, 0.16910004615783691, 0.17070001363754272, 0.17020004987716675, 0.17410004138946533]\n",
            "\n",
            "\n",
            "TIME:  10338.7914932\n",
            "--------------- DONE!!! ----------------------------\n",
            "\n",
            "\n",
            "Namespace(arch='bam_resnet34_s', batch_size=128, epoch=100, learning_rate=0.1)\n",
            "Model:  bam_resnet34_s\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "[Epoch   0], lr: 0.10000, Loss: 4.230 | top1-e-train: 0.879, top5-e-train: 0.646 | top1-e-test: 0.866, top5-e-test: 0.631\n",
            "[Epoch   1], lr: 0.10000, Loss: 3.465 | top1-e-train: 0.787, top5-e-train: 0.489 | top1-e-test: 0.769, top5-e-test: 0.477\n",
            "[Epoch   2], lr: 0.10000, Loss: 3.020 | top1-e-train: 0.709, top5-e-train: 0.391 | top1-e-test: 0.705, top5-e-test: 0.393\n",
            "[Epoch   3], lr: 0.10000, Loss: 2.682 | top1-e-train: 0.649, top5-e-train: 0.324 | top1-e-test: 0.650, top5-e-test: 0.326\n",
            "[Epoch   4], lr: 0.10000, Loss: 2.451 | top1-e-train: 0.606, top5-e-train: 0.274 | top1-e-test: 0.609, top5-e-test: 0.281\n",
            "[Epoch   5], lr: 0.10000, Loss: 2.270 | top1-e-train: 0.607, top5-e-train: 0.283 | top1-e-test: 0.583, top5-e-test: 0.265\n",
            "[Epoch   6], lr: 0.10000, Loss: 2.105 | top1-e-train: 0.548, top5-e-train: 0.232 | top1-e-test: 0.563, top5-e-test: 0.256\n",
            "[Epoch   7], lr: 0.10000, Loss: 1.972 | top1-e-train: 0.492, top5-e-train: 0.183 | top1-e-test: 0.515, top5-e-test: 0.211\n",
            "[Epoch   8], lr: 0.10000, Loss: 1.835 | top1-e-train: 0.476, top5-e-train: 0.174 | top1-e-test: 0.516, top5-e-test: 0.215\n",
            "[Epoch   9], lr: 0.10000, Loss: 1.731 | top1-e-train: 0.456, top5-e-train: 0.160 | top1-e-test: 0.504, top5-e-test: 0.206\n",
            "[Epoch  10], lr: 0.10000, Loss: 1.707 | top1-e-train: 0.462, top5-e-train: 0.160 | top1-e-test: 0.500, top5-e-test: 0.204\n",
            "[Epoch  11], lr: 0.10000, Loss: 1.601 | top1-e-train: 0.423, top5-e-train: 0.136 | top1-e-test: 0.488, top5-e-test: 0.192\n",
            "[Epoch  12], lr: 0.10000, Loss: 1.465 | top1-e-train: 0.400, top5-e-train: 0.118 | top1-e-test: 0.479, top5-e-test: 0.190\n",
            "[Epoch  13], lr: 0.10000, Loss: 1.372 | top1-e-train: 0.348, top5-e-train: 0.087 | top1-e-test: 0.458, top5-e-test: 0.178\n",
            "[Epoch  14], lr: 0.10000, Loss: 1.276 | top1-e-train: 0.323, top5-e-train: 0.078 | top1-e-test: 0.447, top5-e-test: 0.166\n",
            "[Epoch  15], lr: 0.10000, Loss: 1.220 | top1-e-train: 0.377, top5-e-train: 0.108 | top1-e-test: 0.481, top5-e-test: 0.199\n",
            "[Epoch  16], lr: 0.10000, Loss: 1.119 | top1-e-train: 0.303, top5-e-train: 0.067 | top1-e-test: 0.458, top5-e-test: 0.173\n",
            "[Epoch  17], lr: 0.10000, Loss: 1.045 | top1-e-train: 0.281, top5-e-train: 0.060 | top1-e-test: 0.437, top5-e-test: 0.169\n",
            "[Epoch  18], lr: 0.10000, Loss: 1.062 | top1-e-train: 0.267, top5-e-train: 0.051 | top1-e-test: 0.443, top5-e-test: 0.167\n",
            "[Epoch  19], lr: 0.10000, Loss: 0.990 | top1-e-train: 0.271, top5-e-train: 0.052 | top1-e-test: 0.454, top5-e-test: 0.180\n",
            "[Epoch  20], lr: 0.10000, Loss: 0.873 | top1-e-train: 0.296, top5-e-train: 0.080 | top1-e-test: 0.484, top5-e-test: 0.210\n",
            "[Epoch  21], lr: 0.10000, Loss: 0.824 | top1-e-train: 0.201, top5-e-train: 0.029 | top1-e-test: 0.426, top5-e-test: 0.166\n",
            "[Epoch  22], lr: 0.10000, Loss: 0.862 | top1-e-train: 0.211, top5-e-train: 0.032 | top1-e-test: 0.443, top5-e-test: 0.175\n",
            "[Epoch  23], lr: 0.10000, Loss: 0.720 | top1-e-train: 0.242, top5-e-train: 0.051 | top1-e-test: 0.458, top5-e-test: 0.186\n",
            "[Epoch  24], lr: 0.10000, Loss: 0.684 | top1-e-train: 0.159, top5-e-train: 0.016 | top1-e-test: 0.435, top5-e-test: 0.167\n",
            "[Epoch  25], lr: 0.10000, Loss: 0.652 | top1-e-train: 0.143, top5-e-train: 0.016 | top1-e-test: 0.432, top5-e-test: 0.171\n",
            "[Epoch  26], lr: 0.10000, Loss: 0.619 | top1-e-train: 0.148, top5-e-train: 0.015 | top1-e-test: 0.440, top5-e-test: 0.175\n",
            "[Epoch  27], lr: 0.10000, Loss: 0.673 | top1-e-train: 0.129, top5-e-train: 0.009 | top1-e-test: 0.431, top5-e-test: 0.166\n",
            "[Epoch  28], lr: 0.10000, Loss: 0.569 | top1-e-train: 0.142, top5-e-train: 0.014 | top1-e-test: 0.446, top5-e-test: 0.178\n",
            "[Epoch  29], lr: 0.01000, Loss: 0.307 | top1-e-train: 0.057, top5-e-train: 0.004 | top1-e-test: 0.405, top5-e-test: 0.158\n",
            "[Epoch  30], lr: 0.01000, Loss: 0.218 | top1-e-train: 0.045, top5-e-train: 0.004 | top1-e-test: 0.404, top5-e-test: 0.155\n",
            "[Epoch  31], lr: 0.01000, Loss: 0.169 | top1-e-train: 0.037, top5-e-train: 0.003 | top1-e-test: 0.405, top5-e-test: 0.155\n",
            "[Epoch  32], lr: 0.01000, Loss: 0.147 | top1-e-train: 0.033, top5-e-train: 0.003 | top1-e-test: 0.404, top5-e-test: 0.155\n",
            "[Epoch  33], lr: 0.01000, Loss: 0.129 | top1-e-train: 0.026, top5-e-train: 0.003 | top1-e-test: 0.403, top5-e-test: 0.157\n",
            "[Epoch  34], lr: 0.01000, Loss: 0.116 | top1-e-train: 0.026, top5-e-train: 0.004 | top1-e-test: 0.408, top5-e-test: 0.161\n",
            "[Epoch  35], lr: 0.01000, Loss: 0.103 | top1-e-train: 0.021, top5-e-train: 0.002 | top1-e-test: 0.403, top5-e-test: 0.156\n",
            "[Epoch  36], lr: 0.01000, Loss: 0.092 | top1-e-train: 0.021, top5-e-train: 0.004 | top1-e-test: 0.404, top5-e-test: 0.160\n",
            "[Epoch  37], lr: 0.01000, Loss: 0.084 | top1-e-train: 0.020, top5-e-train: 0.003 | top1-e-test: 0.405, top5-e-test: 0.159\n",
            "[Epoch  38], lr: 0.01000, Loss: 0.080 | top1-e-train: 0.020, top5-e-train: 0.006 | top1-e-test: 0.407, top5-e-test: 0.161\n",
            "[Epoch  39], lr: 0.01000, Loss: 0.073 | top1-e-train: 0.017, top5-e-train: 0.004 | top1-e-test: 0.404, top5-e-test: 0.159\n",
            "[Epoch  40], lr: 0.01000, Loss: 0.069 | top1-e-train: 0.017, top5-e-train: 0.004 | top1-e-test: 0.403, top5-e-test: 0.157\n",
            "[Epoch  41], lr: 0.01000, Loss: 0.064 | top1-e-train: 0.014, top5-e-train: 0.004 | top1-e-test: 0.403, top5-e-test: 0.158\n",
            "[Epoch  42], lr: 0.01000, Loss: 0.057 | top1-e-train: 0.012, top5-e-train: 0.002 | top1-e-test: 0.401, top5-e-test: 0.159\n",
            "[Epoch  43], lr: 0.01000, Loss: 0.053 | top1-e-train: 0.012, top5-e-train: 0.003 | top1-e-test: 0.403, top5-e-test: 0.157\n",
            "[Epoch  44], lr: 0.01000, Loss: 0.050 | top1-e-train: 0.014, top5-e-train: 0.005 | top1-e-test: 0.403, top5-e-test: 0.162\n",
            "[Epoch  45], lr: 0.01000, Loss: 0.050 | top1-e-train: 0.010, top5-e-train: 0.002 | top1-e-test: 0.402, top5-e-test: 0.156\n",
            "[Epoch  46], lr: 0.01000, Loss: 0.045 | top1-e-train: 0.009, top5-e-train: 0.001 | top1-e-test: 0.400, top5-e-test: 0.155\n",
            "[Epoch  47], lr: 0.01000, Loss: 0.043 | top1-e-train: 0.010, top5-e-train: 0.002 | top1-e-test: 0.400, top5-e-test: 0.152\n",
            "[Epoch  48], lr: 0.01000, Loss: 0.041 | top1-e-train: 0.012, top5-e-train: 0.004 | top1-e-test: 0.402, top5-e-test: 0.157\n",
            "[Epoch  49], lr: 0.01000, Loss: 0.039 | top1-e-train: 0.009, top5-e-train: 0.002 | top1-e-test: 0.401, top5-e-test: 0.155\n",
            "[Epoch  50], lr: 0.01000, Loss: 0.034 | top1-e-train: 0.009, top5-e-train: 0.002 | top1-e-test: 0.399, top5-e-test: 0.153\n",
            "[Epoch  51], lr: 0.01000, Loss: 0.035 | top1-e-train: 0.008, top5-e-train: 0.002 | top1-e-test: 0.399, top5-e-test: 0.156\n",
            "[Epoch  52], lr: 0.01000, Loss: 0.035 | top1-e-train: 0.009, top5-e-train: 0.003 | top1-e-test: 0.405, top5-e-test: 0.157\n",
            "[Epoch  53], lr: 0.01000, Loss: 0.033 | top1-e-train: 0.006, top5-e-train: 0.001 | top1-e-test: 0.402, top5-e-test: 0.155\n",
            "[Epoch  54], lr: 0.01000, Loss: 0.030 | top1-e-train: 0.009, top5-e-train: 0.003 | top1-e-test: 0.399, top5-e-test: 0.156\n",
            "[Epoch  55], lr: 0.01000, Loss: 0.030 | top1-e-train: 0.007, top5-e-train: 0.001 | top1-e-test: 0.398, top5-e-test: 0.157\n",
            "[Epoch  56], lr: 0.01000, Loss: 0.028 | top1-e-train: 0.006, top5-e-train: 0.001 | top1-e-test: 0.400, top5-e-test: 0.155\n",
            "[Epoch  57], lr: 0.01000, Loss: 0.026 | top1-e-train: 0.004, top5-e-train: 0.000 | top1-e-test: 0.399, top5-e-test: 0.152\n",
            "[Epoch  58], lr: 0.01000, Loss: 0.026 | top1-e-train: 0.005, top5-e-train: 0.001 | top1-e-test: 0.401, top5-e-test: 0.154\n",
            "[Epoch  59], lr: 0.00100, Loss: 0.023 | top1-e-train: 0.007, top5-e-train: 0.003 | top1-e-test: 0.400, top5-e-test: 0.155\n",
            "[Epoch  60], lr: 0.00100, Loss: 0.021 | top1-e-train: 0.006, top5-e-train: 0.002 | top1-e-test: 0.399, top5-e-test: 0.155\n",
            "[Epoch  61], lr: 0.00100, Loss: 0.021 | top1-e-train: 0.006, top5-e-train: 0.001 | top1-e-test: 0.399, top5-e-test: 0.154\n",
            "[Epoch  62], lr: 0.00100, Loss: 0.021 | top1-e-train: 0.007, top5-e-train: 0.003 | top1-e-test: 0.400, top5-e-test: 0.155\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}